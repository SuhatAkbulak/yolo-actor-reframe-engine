# -*- coding: utf-8 -*-
"""Video Resizr.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iwxcG1Sgq8XzL3tnPYfbSxAcNa0nlhHI
"""

!wget -O input_video.mp4 https://www.bigaraj.com/videos/2.mp4

!pip install ultralytics torch opencv-python numpy

import cv2, json, numpy as np
import torch
from ultralytics import YOLO
from collections import defaultdict

# 1. Load Model
device = 'cuda' if torch.cuda.is_available() else 'cpu'
pose_model = YOLO('yolov8n-pose.pt').to(device)

video_path = 'input_video.mp4'
cap = cv2.VideoCapture(video_path)
W, H = int(cap.get(3)), int(cap.get(4))
total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

raw_frames_data = []
scene_id = 0
prev_hist = None

# Global Statistics (For main actor selection)
actor_scores = defaultdict(lambda: {"total_score": 0, "frames_present": 0, "has_face": False})

for f_idx in range(total_frames):
    ret, frame = cap.read()
    if not ret: break

    # --- A. SCENE TRANSITION CHECK ---
    small = cv2.resize(frame, (64, 64))
    hsv = cv2.cvtColor(small, cv2.COLOR_BGR2HSV)
    hist = cv2.calcHist([hsv], [0, 1], None, [32, 32], [0, 180, 0, 256])
    cv2.normalize(hist, hist, 0, 1, cv2.NORM_MINMAX)
    if prev_hist is not None and cv2.compareHist(prev_hist, hist, cv2.HISTCMP_CORREL) < 0.82:
        scene_id += 1
    prev_hist = hist

    # --- B. POSE & FACE DETECTION ---
    results = pose_model.track(frame, persist=True, verbose=False, imgsz=640)

    current_frame_faces = []
    current_frame_objects = []

    if results[0].keypoints is not None and results[0].boxes is not None:
        kpts = results[0].keypoints.xyn.cpu().numpy()
        boxes = results[0].boxes.xyxy.cpu().numpy()
        ids = results[0].boxes.id.cpu().numpy() if results[0].boxes.id is not None else [0]*len(boxes)

        for i, (person_kpts, box, t_id) in enumerate(zip(kpts, boxes, ids)):
            track_id = int(t_id)
            bx1, by1, bx2, by2 = box

            # Object Area and Centrality (Scoring criteria)
            w_pct = (bx2 - bx1) / W
            h_pct = (by2 - by1) / H
            x_center_pct = ((bx1 + bx2)/2 / W) - 0.5
            area = w_pct * h_pct
            centrality = 1.0 - abs(x_center_pct)

            # Frame score: Area and central positioning
            frame_score = area * centrality

            # Face Check
            face_kpts = person_kpts[:5]
            valid_face = face_kpts[face_kpts[:, 0] > 0]
            is_face_present = len(valid_face) > 0

            if is_face_present:
                fx_min, fy_min = np.min(valid_face, axis=0)
                fx_max, fy_max = np.max(valid_face, axis=0)
                current_frame_faces.append({
                    "id": f"face_{track_id}",
                    "pos": {
                        "xPct": float((fx_min + fx_max)/2 - 0.5),
                        "yPct": float((fy_min + fy_max)/2 - 0.5),
                        "wPct": float(fx_max - fx_min),
                        "hPct": float(fy_max - fy_min)
                    }
                })
                actor_scores[track_id]["has_face"] = True
                frame_score *= 2.0  # Boost score if face is present

            # Update Global Score
            actor_scores[track_id]["total_score"] += frame_score
            actor_scores[track_id]["frames_present"] += 1

            current_frame_objects.append({
                "id": track_id,
                "label": "person",
                "pos": {
                    "xPct": float(x_center_pct),
                    "yPct": float(((by1 + by2)/2 / H) - 0.5),
                    "wPct": float(w_pct),
                    "hPct": float(h_pct)
                }
            })

    raw_frames_data.append({
        "frame": f_idx,
        "scene_id": scene_id,
        "faceV2": current_frame_faces,
        "detected_objects": current_frame_objects
    })

    if f_idx % 100 == 0:
        print(f"{f_idx}/{total_frames} frames scanned...")

# --- C. POST-PROCESSING: MAIN CHARACTER DECISION ---

# Sort actors by total score
sorted_actors = sorted(actor_scores.items(), key=lambda x: x[1]['total_score'], reverse=True)
main_actor_id = sorted_actors[0][0] if sorted_actors else None

# Scene-level voting (Find the dominant ID per scene)
scene_leaders = {}
scenes = defaultdict(list)
for f in raw_frames_data:
    scenes[f['scene_id']].append(f)

for s_id, s_frames in scenes.items():
    s_id_counts = defaultdict(float)
    for f in s_frames:
        for obj in f['detected_objects']:
            # If global main actor, give extra weight to maintain scene focus
            weight = 2.0 if obj['id'] == main_actor_id else 1.0
            s_id_counts[obj['id']] += weight

    scene_leaders[s_id] = max(s_id_counts, key=s_id_counts.get) if s_id_counts else None

# Prepare Final Metadata
final_metadata = {
    "video_info": {
        "main_actor_id": main_actor_id,
        "total_scenes": scene_id + 1
    },
    "frames": []
}

for f in raw_frames_data:
    s_leader = scene_leaders[f['scene_id']]

    # Embed "focus coordinate" into each frame
    focus_x = 0.0
    for obj in f['detected_objects']:
        if obj['id'] == s_leader:
            focus_x = obj['pos']['xPct']
            break

    f['render_focus_x'] = focus_x
    f['is_main_actor_present'] = any(obj['id'] == main_actor_id for obj in f['detected_objects'])
    final_metadata["frames"].append(f)

cap.release()
with open('video_analysis.json', 'w') as f:
    json.dump(final_metadata, f, indent=2)

print(f"\nCOMPLETE! Main Actor ID: {main_actor_id}")

import cv2, json, numpy as np
from tqdm import tqdm
import os

# 1. SETTINGS
ANALYSIS_PATH = 'video_analysis.json'
VIDEO_PATH = 'input_video.mp4'
OUTPUT_RAW = 'output_temp.mp4'
OUTPUT_FINAL = 'final_vertical_video_with_audio.mp4'

# Load JSON and Error Handling
if not os.path.exists(ANALYSIS_PATH):
    raise FileNotFoundError(f"Error: {ANALYSIS_PATH} not found! Run the analysis first.")

with open(ANALYSIS_PATH, 'r') as f:
    data = json.load(f)

# Video Information
cap = cv2.VideoCapture(VIDEO_PATH)
fps = cap.get(cv2.CAP_PROP_FPS)
orig_w = int(cap.get(3))
orig_h = int(cap.get(4))

# Output Size (Vertical 9:16)
target_h = orig_h
target_w = int(target_h * 9 / 16)

# Video Writer (mp4v is sufficient for temporary file)
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(OUTPUT_RAW, fourcc, fps, (target_w, target_h))

# Get Main Actor Information from Metadata (If not available, write 'Not Detected')
video_info = data.get('video_info', {})
main_actor = video_info.get('main_actor_id', 'Not Detected')
print(f"Rendering Started... (Focus Target: {main_actor})")

# Scene-based fixed crop tracking
scene_x_cache = {}

# 2. RENDER LOOP
for frame_info in tqdm(data['frames'], desc="Processing Frames"):
    ret, frame = cap.read()
    if not ret: break

    s_id = frame_info['scene_id']

    # If crop coordinate for this scene hasn't been calculated, calculate it
    if s_id not in scene_x_cache:
        # Find all frames belonging to the current scene
        current_scene_frames = [f for f in data['frames'] if f['scene_id'] == s_id]

        # Collect render_focus_x values (If missing, default to 0.0 meaning center)
        scene_x_vals = [f.get('render_focus_x', 0.0) for f in current_scene_frames]

        if len(scene_x_vals) > 0:
            avg_focus_x = np.median(scene_x_vals)
        else:
            avg_focus_x = 0.0  # Default center

        # Convert xPct (-0.5, 0.5) range to pixels
        target_center_x = (avg_focus_x + 0.5) * orig_w

        # Check boundaries (Prevent crop from going out of frame)
        x1 = int(target_center_x - target_w / 2)
        if x1 < 0: x1 = 0
        if x1 + target_w > orig_w: x1 = orig_w - target_w

        scene_x_cache[s_id] = x1

    # Use fixed x1 coordinate for the scene
    x1 = scene_x_cache[s_id]
    x2 = x1 + target_w

    # Cropping operation
    cropped_frame = frame[0:target_h, x1:x2]

    # OpenCV may sometimes cause a 1-pixel mismatch, fix it
    if cropped_frame.shape[1] != target_w:
        cropped_frame = cv2.resize(cropped_frame, (target_w, target_h))

    out.write(cropped_frame)

cap.release()
out.release()

# 3. FFMPEG: Convert video to H.264 + Add Original Audio

# This command:
# - Takes the video from the vertical output (OUTPUT_RAW)
# - Takes the audio from the original video (VIDEO_PATH)
# - Merges everything into a single file
cmd = (
    f"ffmpeg -i {OUTPUT_RAW} -i {VIDEO_PATH} "
    f"-map 0:v -map 1:a? -c:v libx264 -crf 18 -pix_fmt yuv420p "
    f"-c:a aac -b:a 192k -shortest {OUTPUT_FINAL} -y -loglevel quiet"
)

os.system(cmd)

# Cleanup: You can delete the temporary raw file
if os.path.exists(OUTPUT_RAW):
    os.remove(OUTPUT_RAW)

from IPython.display import HTML
from base64 import b64encode

video_file = open('final_vertical_video_with_audio.mp4', 'rb').read()
data_url = "data:video/mp4;base64," + b64encode(video_file).decode()

HTML(f"""
<video width=400 controls>
      <source src="{data_url}" type="video/mp4">
</video>
""")